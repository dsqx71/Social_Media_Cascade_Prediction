{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os.path\n",
    "import access_data \n",
    "import preprocessing\n",
    "import setting\n",
    "import feature\n",
    "import sys\n",
    "import time\n",
    "\n",
    "pd.set_option('display.max_columns' ,1000)\n",
    "pd.set_option('display.max_rows',60)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#清理 + 分词 + label encode+计算词频"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:finished cleaning symbol http[0-9a-zA-Z?:=._@%/\\-#&\\+|]+\n",
      "INFO:root:finished cleaning symbol //@\n",
      "INFO:root:finished cleaning symbol @\n",
      "INFO:root:finished cleaning symbol #\n",
      "INFO:root:finished cleaning symbol 【\n",
      "INFO:root:finished cleaning symbol 《\n",
      "INFO:root:finished cleaning symbol \\[\n",
      "INFO:root:Start to segment\n",
      "Building prefix dict from C:\\Anaconda\\lib\\site-packages\\jieba\\dict.txt ...\n",
      "DEBUG:jieba:Building prefix dict from C:\\Anaconda\\lib\\site-packages\\jieba\\dict.txt ...\n",
      "Loading model from cache c:\\users\\admini~1\\appdata\\local\\temp\\jieba.cache\n",
      "DEBUG:jieba:Loading model from cache c:\\users\\admini~1\\appdata\\local\\temp\\jieba.cache\n",
      "Loading model cost 0.955 seconds.\n",
      "DEBUG:jieba:Loading model cost 0.955 seconds.\n",
      "Prefix dict has been built succesfully.\n",
      "DEBUG:jieba:Prefix dict has been built succesfully.\n",
      "INFO:root:have segmented 0\n",
      "INFO:root:have segmented 100000\n",
      "INFO:root:Start to segment\n",
      "INFO:root:have segmented 0\n",
      "INFO:root:have segmented 100000\n",
      "INFO:root:have segmented 200000\n",
      "INFO:root:have segmented 300000\n",
      "INFO:root:have segmented 400000\n",
      "INFO:root:have segmented 500000\n",
      "INFO:root:have segmented 600000\n",
      "INFO:root:have segmented 700000\n",
      "INFO:root:have segmented 800000\n",
      "INFO:root:have segmented 900000\n",
      "INFO:root:have segmented 1000000\n",
      "INFO:root:have segmented 1100000\n",
      "INFO:root:have segmented 1200000\n",
      "INFO:root:have segmented 1300000\n",
      "INFO:root:have segmented 1400000\n",
      "INFO:root:have segmented 1500000\n",
      "INFO:root:have segmented 1600000\n",
      "INFO:root:have segmented 1700000\n",
      "INFO:root:have segmented 1800000\n",
      "INFO:root:have segmented 1900000\n",
      "INFO:root:have segmented 2000000\n",
      "INFO:root:have segmented 2100000\n",
      "INFO:root:have segmented 2200000\n",
      "INFO:root:have segmented 2300000\n",
      "INFO:root:have segmented 2400000\n",
      "INFO:root:have segmented 2500000\n",
      "INFO:root:have segmented 2600000\n",
      "INFO:root:have segmented 2700000\n",
      "INFO:root:have segmented 2800000\n"
     ]
    }
   ],
   "source": [
    "train_corpus ,test_corpus = preprocessing.clean_corpus()\n",
    "test_corpus['corpus'] = preprocessing.segment_word(test_corpus['corpus'])\n",
    "train_corpus['corpus'] = preprocessing.segment_word(train_corpus['corpus'])\n",
    "\n",
    "train_corpus.to_pickle(setting.processed_data_dir+'cleaned&segment_train')\n",
    "test_corpus.to_pickle(setting.processed_data_dir+'cleaned&segment_test')\n",
    "\n",
    "train_uid,test_uid = preprocessing.encode_label()\n",
    "\n",
    "preprocessing.bag_of_word(train_corpus['corpus'].values,test_corpus['corpus'].values,min_df=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##主题模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run run_lda.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ##构建用户基本特征 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_uid = access_data.load_processed_data('uid&pid_train')\n",
    "test_uid  = access_data.load_processed_data('uid&pid_test')\n",
    "\n",
    "train ,test = access_data.load_raw_data()\n",
    "\n",
    "train_corpus = access_data.load_processed_data('cleaned&segment_train')\n",
    "test_corpus  = access_data.load_processed_data('cleaned&segment_test')                                               \n",
    "train = pd.concat([train_uid,train,train_corpus],axis=1)\n",
    "test  = pd.concat([test_uid,test,test_corpus],axis=1)\n",
    "\n",
    "train.drop([0,1],axis=1,inplace=True)\n",
    "test.drop([0,1],axis=1,inplace=True)\n",
    "\n",
    "train.columns = ['pid','uid','time','share','comment','zan','raw_corpus','clean&segment','链接','//@','@','#','【','《','\\[']\n",
    "test.columns = ['pid','uid','time','raw_corpus','clean&segment','链接','//@','@','#','【','《','\\[']\n",
    "train['uid'] = train['uid'].astype(np.uint16)\n",
    "test['uid']  = test['uid'].astype(np.uint16)\n",
    "l = ['链接','//@','@','#','【','《','\\[']\n",
    "\n",
    "for string in l :\n",
    "    train[string] = train[string].astype(np.int8)\n",
    "    test[string]  = test[string].astype(np.int8)\n",
    "\n",
    "#在training set和test set中和用户发送微博的总数量\n",
    "tot = pd.concat([pd.DataFrame(train['uid']),pd.DataFrame(test['uid'])])\n",
    "c = pd.DataFrame(tot['uid'].value_counts())\n",
    "c.columns = ['tot_counts']\n",
    "train = train.merge(c,left_on='uid',right_index=True,how='left')\n",
    "test  = test.merge(c,left_on='uid',right_index=True,how='left')\n",
    "\n",
    "# 用户出现在训练集的次数\n",
    "c = pd.DataFrame(train['uid'].value_counts())\n",
    "c.columns = ['train_counts']\n",
    "train = train.merge(c,left_on='uid',right_index=True,how='left')\n",
    "test  = test.merge(c,left_on='uid',right_index=True,how='left')\n",
    "\n",
    "test.fillna(-1,inplace=True)\n",
    "train['tot_counts'] = train['tot_counts'].astype(np.int32)\n",
    "train['train_counts'] = train['train_counts'].astype(np.int32)\n",
    "\n",
    "test['tot_counts'] = test['tot_counts'].astype(np.int32)\n",
    "test['train_counts'] = test['train_counts'].astype(np.int32)\n",
    "\n",
    "addr1 = setting.raw_data_dir + 'basic_train'\n",
    "addr2 = setting.raw_data_dir + 'basic_test'\n",
    "\n",
    "lda_result = np.load('processed_data/lda_result_version3.npy')\n",
    "lda_result = pd.DataFrame(lda_result,columns=['topic_%d' %i for i in range(0,25)])\n",
    "\n",
    "for string in ['topic_%d' %i for i in range(0,25)]:\n",
    "    train[string] = lda_result.loc[:train.shape[0]-1,string].values\n",
    "    test[string] = lda_result.loc[train.shape[0]:,string].values\n",
    "\n",
    "#train.to_pickle(addr1)\n",
    "#test.to_pickle(addr2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##用户特征 + 时间特征  + 文本特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train_basic = pd.read_pickle('raw_data/basic_train')\n",
    "#test_basic  = pd.read_pickle('raw_data/basic_test')\n",
    "#train_basic = train_basic.loc[1626750:]\n",
    "#计算情感极性\n",
    "begin_time= time.time()\n",
    "train_sentiment,test_sentiment = feature.sentiment_feature(train_basic,test_basic)\n",
    "end_time = time.time()\n",
    "print end_time - begin_time\n",
    "#计算一周内出现微博的数量\n",
    "begin_time= time.time()\n",
    "train_seven_days ,test_seven_days = feature.find_seven_days(train_basic,test_basic)\n",
    "end_time = time.time()\n",
    "print end_time - begin_time\n",
    "#lda特征\n",
    "begin_time = time.time()\n",
    "train_lda_feature,test_lda_feature = feature.lda_feature(train_basic,test_basic)\n",
    "end_time = time.time()\n",
    "print end_time - begin_time\n",
    "#用户特征\n",
    "begin_time= time.time()\n",
    "train_user,test_user = feature.user_basic_feature(train_basic,test_basic)\n",
    "end_time = time.time()\n",
    "print end_time - begin_time\n",
    "#文本特征\n",
    "begin_time= time.time()\n",
    "train_content,test_content = feature.content_basic_feature(train_basic,test_basic)\n",
    "end_time = time.time()\n",
    "print end_time - begin_time\n",
    "#时间特征\n",
    "begin_time= time.time()\n",
    "train_time,test_time = feature.time_feature(train_basic,test_basic)\n",
    "end_time = time.time()\n",
    "print end_time - begin_time\n",
    "#关键词特征\n",
    "begin_time = time.time()\n",
    "train_keyword,test_keyword = feature.key_word_feature(train_basic,test_basic)\n",
    "end_time = time.time()\n",
    "print end_time - begin_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_basic.drop(['uid','pid','time','share','comment','zan','raw_corpus','clean&segment'],axis=1,inplace=True)\n",
    "test_basic.drop(['uid','pid','time','raw_corpus','clean&segment'],axis=1,inplace=True)\n",
    "train_user = pd.concat([train_user,train_basic],axis=1)\n",
    "test_user  = pd.concat([test_user,test_basic],axis=1)\n",
    "\n",
    "train = train_user.merge(train_content,how='left',left_on='pid',right_index=True)\n",
    "test  = test_user.merge(test_content,how='left',left_on='pid',right_index=True)\n",
    "\n",
    "train = train.merge(train_time,how='left',left_on='pid',right_index=True)\n",
    "test  = test.merge(test_time,how='left',left_on='pid',right_index=True)\n",
    "\n",
    "train = pd.concat([train,train_keyword],axis=1)\n",
    "test  = pd.concat([test,test_keyword],axis=1)\n",
    "\n",
    "train = train.merge(train_lda_feature,how='left',left_on='pid',right_index=True)\n",
    "test  = test.merge(test_lda_feature,how='left',left_on='pid',right_index=True)\n",
    "\n",
    "train = train.merge(train_sentiment,how='left',left_on='pid',right_index=True)\n",
    "test  = test.merge(test_sentiment,how='left',left_on='pid',right_index=True)\n",
    "\n",
    "train = train.merge(train_seven_days,how='left',left_on='pid',right_index=True)\n",
    "test  = test.merge(test_seven_days,how='left',left_on='pid',right_index=True)\n",
    "\n",
    "begin_time = time.time()\n",
    "result,gmm = feature.clustering_feature(train,test)\n",
    "end_time = time.time()\n",
    "print end_time - begin_time\n",
    "\n",
    "train = train.merge(result,how='left',left_on='uid',right_index=True)\n",
    "test  = test.merge(result,how='left',left_on='uid',right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##整理格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train.drop(['sentiment','seven_days'],axis=1,inplace=True)\n",
    "#test.drop(['sentiment','seven_days'],axis=1,inplace=True)\n",
    "result_test = []\n",
    "result_train = []\n",
    "tot = 0\n",
    "for string in ['share','comment','zan','content_len','链接','//@','@','#','【','《','\\[']:\n",
    "    temp = []\n",
    "    for i in test[string+'_histogram']:\n",
    "        if isinstance(i,int):\n",
    "            temp.append(np.zeros(shape=8))\n",
    "            tot +=1\n",
    "        else:\n",
    "            temp.append(i[0])\n",
    "    result_test.append(np.asarray(temp))\n",
    "    temp = []\n",
    "    for i in train[string+'_histogram']:\n",
    "        temp.append(i[0])\n",
    "    result_train.append(np.asarray(temp))\n",
    "    \n",
    "    train.drop(string+'_histogram',axis=1,inplace=True)\n",
    "    test.drop(string+'_histogram',axis=1,inplace=True)\n",
    "train.drop(['pid','uid'],inplace=True,axis = 1)\n",
    "test.drop(['pid','uid'],inplace=True,axis = 1)\n",
    "\n",
    "train_y = train[['share','comment','zan']].values\n",
    "train.drop(['share','comment','zan'],axis = 1,inplace=True)\n",
    "train_x = train.values\n",
    "test_x  = test.values\n",
    "for i in result_train:\n",
    "    train_x = np.c_[train_x,i]\n",
    "for i in result_test:\n",
    "    test_x = np.c_[test_x,i]\n",
    "np.save('processed_data/train3_np',train_x)\n",
    "np.save('processed_data/test3_np',test_x)\n",
    "np.save('processed_data/target3_np',train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run learning.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
